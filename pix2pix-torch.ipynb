{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BASIC_D(nn.Module):\n",
    "    def __init__(self, nc_in, nc_out, ndf, max_layers=3, ngpu=1):\n",
    "        super(BASIC_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.nc_in = nc_in\n",
    "        self.nc_out = nc_out        \n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc_in+nc_out, ndf),\n",
    "                        nn.Conv2d(nc_in+nc_out, ndf, 4, 2, 1))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        out_feat = ndf\n",
    "        for layer in range(1, max_layers):\n",
    "            in_feat = out_feat\n",
    "            out_feat = ndf * min(2**layer, 8)\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))            \n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "        in_feat = out_feat\n",
    "        out_feat = ndf*min(2**max_layers, 8)\n",
    "        main.add_module('last.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 1, 1, bias=False))\n",
    "        main.add_module('last.{0}.batchnorm'.format(out_feat), nn.BatchNorm2d(out_feat))\n",
    "        main.add_module('last.{0}.relu'.format(out_feat), nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        in_feat, out_feat = out_feat, 1        \n",
    "        main.add_module('final.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 1, 1, bias=True))\n",
    "        main.add_module('final.{0}.sigmoid'.format(out_feat),  nn.nn.Sigmoid())        \n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        x = torch.cat((a, b), 1)        \n",
    "        output = self.main(x)                    \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class UNET_G(nn.Module):\n",
    "    def __init__(self, isize, nc_in=3, nc_out=3, ngf=64):\n",
    "        super(UNET_G, self).__init__()       \n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "        conv_layers = []\n",
    "        convt_layers = []\n",
    "        # Down sampling\n",
    "        tsize = isize\n",
    "        out_feat = nc_in\n",
    "        layers = []\n",
    "        while True:\n",
    "            assert tsize>=2 and tsize%2==0\n",
    "            out_feat, in_feat = ngf * min(2**len(conv_layers), 8), out_feat\n",
    "            use_batchnorm = len(conv_layers)>1 and tsize>2\n",
    "            layers.append(nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=not use_batchnorm))\n",
    "            if tsize==2:\n",
    "                break \n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_feat))\n",
    "            conv_layers.append(nn.Sequential(*layers))\n",
    "            layers = [nn.LeakyReLU(0.2, inplace=True)]            \n",
    "            tsize = tsize // 2\n",
    "        \n",
    "        # Up sampling\n",
    "        use_batchnorm = True\n",
    "        while tsize<isize:\n",
    "            layers.append(nn.ReLU)\n",
    "            out_feat, in_feat = ngf * min(2**(len(conv_layers)-1), 8), out_feat\n",
    "            layers.append(nn.ConvTranspose2d(in_feat, out_feat,\n",
    "                                        kernel_size=4, stride=2,padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_feat))\n",
    "            if tsize <=8:\n",
    "                layers.append(DropoutLayer(0.5))\n",
    "            convt_layers.append(layers)\n",
    "            layers = []\n",
    "            out_feat = out_feat*2\n",
    "        self.output = nn.Sequential(nn.ReLU, \n",
    "                      nn.ConvTranspose2d(out_feat, 1, kernel_size=4, stride=2,padding=1, bias=True),\n",
    "                      nn.Tanh()\n",
    "                      )\n",
    "        self.conv_layers = nn.ModuleList((conv_layers)\n",
    "        self.convt_layers = nn.ModuleList(convt_layers)\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for l in self.conv_layers:\n",
    "            x = l(x)\n",
    "            outputs.append(x)\n",
    "        for l in self.convt_layers:\n",
    "            x = l(x)            \n",
    "            x = torch.cat((x, outputs.pop()), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc_in = 3\n",
    "nc_out = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "Î» = 10\n",
    "\n",
    "loadSize = 286\n",
    "imageSize = 256\n",
    "batchSize = 1\n",
    "lrD = 2e-4\n",
    "lrG = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netD = BASIC_D(nc_in, nc_out, ndf)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netG = UNET_G(imageSize, nc_in, nc_out, ngf)\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(batchSize, 3, imageSize, imageSize)\n",
    "noise = torch.FloatTensor(batchSize, nz, 1, 1)\n",
    "fixed_noise = torch.FloatTensor(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "one = torch.FloatTensor([1])\n",
    "mone = one * -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, mone = one.cuda(), mone.cuda()\n",
    "noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.RMSprop(netD.parameters(), lr = lrD)\n",
    "optimizerG = optim.RMSprop(netG.parameters(), lr = lrG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "niter = 1000\n",
    "gen_iterations = 0\n",
    "for epoch in range(niter):\n",
    "    i = 0\n",
    "    batches = train_X.shape[0]//batchSize\n",
    "    while i < batches:\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        if gen_iterations < 25 or gen_iterations %500 == 0:\n",
    "            _Diters = 100\n",
    "        else:\n",
    "            _Diters = Diters\n",
    "        j = 0\n",
    "        while j < _Diters and i < batches:\n",
    "            j+=1\n",
    "            \n",
    "            # clamp parameters to a cube\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(clamp_lower, clamp_upper)\n",
    "                \n",
    "            real_data = torch.from_numpy(\n",
    "                np.moveaxis(train_X[i*batchSize:(i+1)*batchSize], 3,1)\n",
    "            ).cuda()\n",
    "            i+=1\n",
    "            \n",
    "            netD.zero_grad()\n",
    "            input.resize_as_(real_data).copy_(real_data)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            errD_real = netD(inputv)\n",
    "            errD_real.backward(one)\n",
    "            \n",
    "            # train with fake\n",
    "            noise.resize_(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD(inputv)\n",
    "            errD_fake.backward(mone)\n",
    "            errD = errD_real - errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        noise.resize_(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake)\n",
    "        errG.backward(one)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        if gen_iterations%500 ==0:\n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, niter, i, batches, gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]), time.time()-t0)\n",
    "        if gen_iterations%500 == 0:            \n",
    "            fake = netG(Variable(fixed_noise, volatile=True))            \n",
    "            showX(np.moveaxis(fake.data.cpu().numpy(),1,3), 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
