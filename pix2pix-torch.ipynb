{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BASIC_D(nn.Module):\n",
    "    def __init__(self, nc_in, nc_out, ndf, max_layers=3, ngpu=1):\n",
    "        super(BASIC_D, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.nc_in = nc_in\n",
    "        self.nc_out = nc_out        \n",
    "        main = nn.Sequential()\n",
    "        # input is nc x isize x isize\n",
    "        main.add_module('initial.conv.{0}-{1}'.format(nc_in+nc_out, ndf),\n",
    "                        nn.Conv2d(nc_in+nc_out, ndf, 4, 2, 1))\n",
    "        main.add_module('initial.relu.{0}'.format(ndf),\n",
    "                        nn.LeakyReLU(0.2, inplace=True))\n",
    "        out_feat = ndf\n",
    "        for layer in range(1, max_layers):\n",
    "            in_feat = out_feat\n",
    "            out_feat = ndf * min(2**layer, 8)\n",
    "            main.add_module('pyramid.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=False))            \n",
    "            main.add_module('pyramid.{0}.batchnorm'.format(out_feat),\n",
    "                            nn.BatchNorm2d(out_feat))\n",
    "            main.add_module('pyramid.{0}.relu'.format(out_feat),\n",
    "                            nn.LeakyReLU(0.2, inplace=True))\n",
    "        in_feat = out_feat\n",
    "        out_feat = ndf*min(2**max_layers, 8)\n",
    "        main.add_module('last.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 1, 1, bias=False))\n",
    "        main.add_module('last.{0}.batchnorm'.format(out_feat), nn.BatchNorm2d(out_feat))\n",
    "        main.add_module('last.{0}.relu'.format(out_feat), nn.LeakyReLU(0.2, inplace=True))\n",
    "        \n",
    "        in_feat, out_feat = out_feat, 1        \n",
    "        main.add_module('final.{0}-{1}.conv'.format(in_feat, out_feat),\n",
    "                            nn.Conv2d(in_feat, out_feat, 4, 1, 1, bias=True))\n",
    "        main.add_module('final.{0}.sigmoid'.format(out_feat),  nn.Sigmoid())        \n",
    "        self.main = main\n",
    "\n",
    "    def forward(self, a, b):\n",
    "        x = torch.cat((a, b), 1)        \n",
    "        output = self.main(x)                    \n",
    "        return output\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class UNET_G(nn.Module):\n",
    "    def __init__(self, isize, nc_in=3, nc_out=3, ngf=64):\n",
    "        super(UNET_G, self).__init__()       \n",
    "        assert isize % 16 == 0, \"isize has to be a multiple of 16\"\n",
    "        conv_layers = []\n",
    "        convt_layers = []\n",
    "        # Down sampling\n",
    "        tsize = isize\n",
    "        out_feat = nc_in\n",
    "        layers = []\n",
    "        while True:\n",
    "            assert tsize>=2 and tsize%2==0\n",
    "            print(\"tsize\", tsize)\n",
    "            out_feat, in_feat = ngf * min(2**len(conv_layers), 8), out_feat\n",
    "            use_batchnorm = len(conv_layers)>1 and tsize>2\n",
    "            layers.append(nn.Conv2d(in_feat, out_feat, 4, 2, 1, bias=not use_batchnorm))\n",
    "            if tsize==2:\n",
    "                break \n",
    "            if use_batchnorm:\n",
    "                layers.append(nn.BatchNorm2d(out_feat))\n",
    "            conv_layers.append(nn.Sequential(*layers))\n",
    "            layers = [nn.LeakyReLU(0.2, inplace=True)]            \n",
    "            tsize = tsize // 2\n",
    "        \n",
    "        # Up sampling\n",
    "        use_batchnorm = True\n",
    "        while tsize<isize:\n",
    "            layers.append(nn.ReLU())\n",
    "            out_feat, in_feat = ngf * min(2**(len(conv_layers)-1), 8), out_feat\n",
    "            layers.append(nn.ConvTranspose2d(in_feat, out_feat,\n",
    "                                        kernel_size=4, stride=2,padding=1, bias=False))\n",
    "            layers.append(nn.BatchNorm2d(out_feat))\n",
    "            if tsize <=8:\n",
    "                layers.append(nn.Dropout(0.5))\n",
    "            convt_layers.append(nn.Sequential(*layers))\n",
    "            layers = []\n",
    "            out_feat = out_feat*2\n",
    "            tsize*=2\n",
    "        self.output = nn.Sequential(nn.ReLU(), \n",
    "                      nn.ConvTranspose2d(out_feat, 1, kernel_size=4, stride=2,padding=1, bias=True),\n",
    "                      nn.Tanh()\n",
    "                      )\n",
    "        self.conv_layers = nn.ModuleList(conv_layers)\n",
    "        self.convt_layers = nn.ModuleList(convt_layers)\n",
    " \n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for l in self.conv_layers:\n",
    "            x = l(x)\n",
    "            outputs.append(x)\n",
    "        for l in self.convt_layers:\n",
    "            x = l(x)            \n",
    "            x = torch.cat((x, outputs.pop()), 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        m.weight.data.normal_(0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02)\n",
    "        m.bias.data.fill_(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc_in = 3\n",
    "nc_out = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "Î» = 10\n",
    "\n",
    "loadSize = 286\n",
    "imageSize = 256\n",
    "batchSize = 1\n",
    "lrD = 2e-4\n",
    "lrG = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BASIC_D (\n",
       "  (main): Sequential (\n",
       "    (initial.conv.6-64): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (initial.relu.64): LeakyReLU (0.2, inplace)\n",
       "    (pyramid.64-128.conv): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid.128.batchnorm): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid.128.relu): LeakyReLU (0.2, inplace)\n",
       "    (pyramid.128-256.conv): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "    (pyramid.256.batchnorm): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (pyramid.256.relu): LeakyReLU (0.2, inplace)\n",
       "    (last.256-512.conv): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (last.512.batchnorm): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    (last.512.relu): LeakyReLU (0.2, inplace)\n",
       "    (final.512-1.conv): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1))\n",
       "    (final.1.sigmoid): Sigmoid ()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "netD = BASIC_D(nc_in, nc_out, ndf)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tsize 256\n",
      "tsize 128\n",
      "tsize 64\n",
      "tsize 32\n",
      "tsize 16\n",
      "tsize 8\n",
      "tsize 4\n",
      "tsize 2\n"
     ]
    }
   ],
   "source": [
    "netG = UNET_G(imageSize, nc_in, nc_out, ngf)\n",
    "netG.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UNET_G (\n",
       "  (output): Sequential (\n",
       "    (0): ReLU ()\n",
       "    (1): ConvTranspose2d(1024, 1, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    (2): Tanh ()\n",
       "  )\n",
       "  (conv_layers): ModuleList (\n",
       "    (0): Sequential (\n",
       "      (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (1): Sequential (\n",
       "      (0): LeakyReLU (0.2, inplace)\n",
       "      (1): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "    )\n",
       "    (2): Sequential (\n",
       "      (0): LeakyReLU (0.2, inplace)\n",
       "      (1): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (3): Sequential (\n",
       "      (0): LeakyReLU (0.2, inplace)\n",
       "      (1): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (4): Sequential (\n",
       "      (0): LeakyReLU (0.2, inplace)\n",
       "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (5): Sequential (\n",
       "      (0): LeakyReLU (0.2, inplace)\n",
       "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (6): Sequential (\n",
       "      (0): LeakyReLU (0.2, inplace)\n",
       "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       "  (convt_layers): ModuleList (\n",
       "    (0): Sequential (\n",
       "      (0): LeakyReLU (0.2, inplace)\n",
       "      (1): Conv2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1))\n",
       "      (2): ReLU ()\n",
       "      (3): ConvTranspose2d(512, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (5): Dropout (p = 0.5)\n",
       "    )\n",
       "    (1): Sequential (\n",
       "      (0): ReLU ()\n",
       "      (1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (3): Dropout (p = 0.5)\n",
       "    )\n",
       "    (2): Sequential (\n",
       "      (0): ReLU ()\n",
       "      (1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "      (3): Dropout (p = 0.5)\n",
       "    )\n",
       "    (3): Sequential (\n",
       "      (0): ReLU ()\n",
       "      (1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (4): Sequential (\n",
       "      (0): ReLU ()\n",
       "      (1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (5): Sequential (\n",
       "      (0): ReLU ()\n",
       "      (1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "    (6): Sequential (\n",
       "      (0): ReLU ()\n",
       "      (1): ConvTranspose2d(1024, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "inputA = torch.FloatTensor(batchSize, nc_in, imageSize, imageSize)\n",
    "inputB = torch.FloatTensor(batchSize, nc_out, imageSize, imageSize)\n",
    "one = torch.FloatTensor([1])\n",
    "zero = torch.FloatTensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netD.cuda()\n",
    "netG.cuda()\n",
    "input = input.cuda()\n",
    "one, zero = one.cuda(), zero.cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(netD.parameters(), lr = lrD, betas=(0.5, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lrG, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data_A' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-85512e26b0f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBCELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0minputA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_A\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0minputv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_D_real\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnetD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data_A' is not defined"
     ]
    }
   ],
   "source": [
    "netD.zero_grad()\n",
    "loss = nn.BCELoss()\n",
    "inputA.resize_as_(data_A).copy_(data_A)\n",
    "inputv = Variable(input)\n",
    "output_D_real = netD(inputv)\n",
    "errD_real = loss(output_D_real, one) \n",
    "errD_real.backward(one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "niter = 1000\n",
    "gen_iterations = 0\n",
    "for epoch in range(niter):\n",
    "    i = 0\n",
    "    batches = train_X.shape[0]//batchSize\n",
    "    while i < batches:\n",
    "        for p in netD.parameters(): # reset requires_grad\n",
    "            p.requires_grad = True # they are set to False below in netG update\n",
    "        if gen_iterations < 25 or gen_iterations %500 == 0:\n",
    "            _Diters = 100\n",
    "        else:\n",
    "            _Diters = Diters\n",
    "        j = 0\n",
    "        while j < _Diters and i < batches:\n",
    "            j+=1\n",
    "            \n",
    "            # clamp parameters to a cube\n",
    "            for p in netD.parameters():\n",
    "                p.data.clamp_(clamp_lower, clamp_upper)\n",
    "                \n",
    "            real_data = torch.from_numpy(\n",
    "                np.moveaxis(train_X[i*batchSize:(i+1)*batchSize], 3,1)\n",
    "            ).cuda()\n",
    "            i+=1\n",
    "            \n",
    "            netD.zero_grad()\n",
    "            input.resize_as_(real_data).copy_(real_data)\n",
    "            inputv = Variable(input)\n",
    "            \n",
    "            errD_real = netD(inputv)\n",
    "            errD_real.backward(one)\n",
    "            \n",
    "            # train with fake\n",
    "            noise.resize_(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "            noisev = Variable(noise, volatile = True) # totally freeze netG\n",
    "            fake = Variable(netG(noisev).data)\n",
    "            inputv = fake\n",
    "            errD_fake = netD(inputv)\n",
    "            errD_fake.backward(mone)\n",
    "            errD = errD_real - errD_fake\n",
    "            optimizerD.step()\n",
    "\n",
    "        for p in netD.parameters():\n",
    "            p.requires_grad = False # to avoid computation\n",
    "        netG.zero_grad()\n",
    "        noise.resize_(batchSize, nz, 1, 1).normal_(0, 1)\n",
    "        noisev = Variable(noise)\n",
    "        fake = netG(noisev)\n",
    "        errG = netD(fake)\n",
    "        errG.backward(one)\n",
    "        optimizerG.step()\n",
    "        gen_iterations += 1\n",
    "        if gen_iterations%500 ==0:\n",
    "            print('[%d/%d][%d/%d][%d] Loss_D: %f Loss_G: %f Loss_D_real: %f Loss_D_fake %f'\n",
    "            % (epoch, niter, i, batches, gen_iterations,\n",
    "            errD.data[0], errG.data[0], errD_real.data[0], errD_fake.data[0]), time.time()-t0)\n",
    "        if gen_iterations%500 == 0:            \n",
    "            fake = netG(Variable(fixed_noise, volatile=True))            \n",
    "            showX(np.moveaxis(fake.data.cpu().numpy(),1,3), 4)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
