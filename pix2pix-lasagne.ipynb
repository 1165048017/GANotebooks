{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,optimizer=fast_run,dnn.library_path=/usr/lib'\n",
    "channel_first = True\n",
    "channel_axis=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cuDNN version 7002 on context None\n",
      "Mapped name None to device cuda: GeForce GTX 1080 (0000:01:00.0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import DropoutLayer, ReshapeLayer, InputLayer \n",
    "floatX = theano.config.floatX\n",
    "from lasagne.layers import Conv2DLayer, TransposedConv2DLayer, ConcatLayer, NonlinearityLayer\n",
    "from lasagne.layers import batch_norm\n",
    "from lasagne.nonlinearities import LeakyRectify, sigmoid, rectify, tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "conv_init = lasagne.init.Normal(0.02, 0)\n",
    "gamma_init = lasagne.init.Normal(0.02, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def BASIC_D(nc_in, nc_out, ndf, max_layers=3, use_sigmoid=True):\n",
    "    l = -1\n",
    "    def conv2d(x, nf, stride=2, nonlinearity=LeakyRectify(0.2)):\n",
    "        nonlocal l\n",
    "        l+=1\n",
    "        return Conv2DLayer(x, num_filters=nf, filter_size=4, stride=stride, \n",
    "                           pad=1, W=conv_init, flip_filters=False,\n",
    "                            nonlinearity=nonlinearity,\n",
    "                           name=\"conv2d_{}\".format(l)\n",
    "                          )\n",
    "    input_a = InputLayer(shape=(None, nc_in, None, None), name=\"inputA\")\n",
    "    input_b = InputLayer(shape=(None, nc_out, None, None), name=\"inputB\")\n",
    "    _ = ConcatLayer([input_a, input_b], name='concat')\n",
    "    _ = conv2d(_, ndf)\n",
    "    for layer in range(1, max_layers):        \n",
    "        out_feat = ndf * min(2**layer, 8)\n",
    "        _ = conv2d(_, out_feat)\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "    out_feat = ndf*min(2**max_layers, 8)\n",
    "    _ = conv2d(_, out_feat, stride=1)\n",
    "    _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)         \n",
    "    _ = conv2d(_, 1, stride=1, nonlinearity=sigmoid if use_sigmoid else None)\n",
    "    return _\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from https://gist.github.com/ajbrock/a3858c26282d9731191901b397b3ce9f\n",
    "def reflect_pad(x, width, batch_ndim=1):\n",
    "    \"\"\"\n",
    "    Pad a tensor with a constant value.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : tensor\n",
    "    width : int, iterable of int, or iterable of tuple\n",
    "        Padding width. If an int, pads each axis symmetrically with the same\n",
    "        amount in the beginning and end. If an iterable of int, defines the\n",
    "        symmetric padding width separately for each axis. If an iterable of\n",
    "        tuples of two ints, defines a seperate padding width for each beginning\n",
    "        and end of each axis.\n",
    "    batch_ndim : integer\n",
    "        Dimensions before the value will not be padded.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Idea for how to make this happen: Flip the tensor horizontally to grab horizontal values, then vertically to grab vertical values\n",
    "    # alternatively, just slice correctly\n",
    "    input_shape = x.shape\n",
    "    input_ndim = x.ndim\n",
    "\n",
    "    output_shape = list(input_shape)\n",
    "    indices = [slice(None) for _ in output_shape]\n",
    "\n",
    "    if isinstance(width, int):\n",
    "        widths = [width] * (input_ndim - batch_ndim)\n",
    "    else:\n",
    "        widths = width\n",
    "\n",
    "    for k, w in enumerate(widths):\n",
    "        try:\n",
    "            l, r = w\n",
    "        except TypeError:\n",
    "            l = r = w\n",
    "        output_shape[k + batch_ndim] += l + r\n",
    "        indices[k + batch_ndim] = slice(l, l + input_shape[k + batch_ndim])\n",
    "\n",
    "    # Create output array\n",
    "    out = T.zeros(output_shape)\n",
    "    \n",
    "    # Vertical Reflections\n",
    "    out=T.set_subtensor(out[:,:,:width,width:-width], x[:,:,width:0:-1,:])# out[:,:,:width,width:-width] = x[:,:,width:0:-1,:]\n",
    "    out=T.set_subtensor(out[:,:,-width:,width:-width], x[:,:,-2:-(2+width):-1,:])#out[:,:,-width:,width:-width] = x[:,:,-2:-(2+width):-1,:]\n",
    "    \n",
    "    # Place X in out\n",
    "    # out = T.set_subtensor(out[tuple(indices)], x) # or, alternative, out[width:-width,width:-width] = x\n",
    "    out=T.set_subtensor(out[:,:,width:-width,width:-width],x)#out[:,:,width:-width,width:-width] = x\n",
    "   \n",
    "   #Horizontal reflections\n",
    "    out=T.set_subtensor(out[:,:,:,:width],out[:,:,:,(2*width):width:-1])#out[:,:,:,:width] = out[:,:,:,(2*width):width:-1]\n",
    "    out=T.set_subtensor(out[:,:,:,-width:],out[:,:,:,-(width+2):-(2*width+2):-1])#out[:,:,:,-width:] = out[:,:,:,-(width+2):-(2*width+2):-1]\n",
    "    \n",
    "    \n",
    "    return out\n",
    "    \n",
    "class ReflectLayer(lasagne.layers.Layer):\n",
    "\n",
    "    def __init__(self, incoming, width, batch_ndim=2, **kwargs):\n",
    "        super(ReflectLayer, self).__init__(incoming, **kwargs)\n",
    "        self.width = width\n",
    "        self.batch_ndim = batch_ndim\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        output_shape = list(input_shape)\n",
    "\n",
    "        if isinstance(self.width, int):\n",
    "            widths = [self.width] * (len(input_shape) - self.batch_ndim)\n",
    "        else:\n",
    "            widths = self.width\n",
    "\n",
    "        for k, w in enumerate(widths):\n",
    "            if output_shape[k + self.batch_ndim] is None:\n",
    "                continue\n",
    "            else:\n",
    "                try:\n",
    "                    l, r = w\n",
    "                except TypeError:\n",
    "                    l = r = w\n",
    "                output_shape[k + self.batch_ndim] += l + r\n",
    "        return tuple(output_shape)\n",
    "\n",
    "    def get_output_for(self, input, **kwargs):\n",
    "        return reflect_pad(input, self.width, self.batch_ndim) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UNET_G(isize, nc_in=3, nc_out=3, ngf=64, fixed_input_size=True):    \n",
    "    max_nf = 8*ngf    \n",
    "    def block(x, s, nf_in, use_batchnorm=True, nf_out=None, nf_next=None):\n",
    "        # print(\"block\",x,s,nf_in, use_batchnorm, nf_out, nf_next)\n",
    "        assert s>=2 and s%2==0\n",
    "        if nf_next is None:\n",
    "            nf_next = min(nf_in*2, max_nf)\n",
    "        if nf_out is None:\n",
    "            nf_out = nf_in\n",
    "            \n",
    "        x = Conv2DLayer(x, num_filters=nf_next, filter_size=4, stride=2, pad=1, W=conv_init, flip_filters=False,                \n",
    "                nonlinearity=None, name='conv2d_{}'.format(s))\n",
    "        if s>2:\n",
    "            if use_batchnorm:\n",
    "                x = batch_norm(x, epsilon=1e-5, gamma=gamma_init)\n",
    "            x2 = NonlinearityLayer(x, nonlinearity=LeakyRectify(0.2), name=\"leakyRelu_{}\".format(s))\n",
    "            x2 = block(x2, s//2, nf_next)\n",
    "            x = ConcatLayer([x, x2], name=\"concat_{}\".format(s))            \n",
    "        x = NonlinearityLayer(x, nonlinearity=rectify, name=\"Relu_{}\".format(s))\n",
    "        x = TransposedConv2DLayer(x, num_filters=nf_out, filter_size=4, stride=2, crop=1, W=conv_init, \n",
    "                                  flip_filters=True, nonlinearity=None, name=\"convt_{}\".format(s))\n",
    "        if use_batchnorm:\n",
    "            x = batch_norm(x, epsilon=1e-5, gamma=gamma_init)\n",
    "        if s <= 8:\n",
    "            x = DropoutLayer(x, 0.5, name=\"dropout_{}\".format(s))\n",
    "        return x\n",
    "    \n",
    "    s = isize if fixed_input_size else None\n",
    "    _ = InputLayer(shape=(None, nc_in, s, s), name='input')\n",
    "    _ = block(_, isize, nc_in, False, nf_out=nc_out, nf_next=ngf)\n",
    "    _ = NonlinearityLayer(_, nonlinearity=tanh, name='tanh')\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from lasagne.layers import ElemwiseSumLayer, SliceLayer\n",
    "def reflect_padding_conv(_, num_filters, filter_size=3, stride=1, nonlinearity=rectify, use_batchnorm=True, **k):\n",
    "    assert filter_size%2==1\n",
    "    pad_size = filter_size>>1\n",
    "    _ = ReflectLayer(_, width=pad_size)\n",
    "    _ = Conv2DLayer(_, num_filters=num_filters, filter_size=filter_size, stride=stride, \n",
    "                           pad=0, W=conv_init, flip_filters=False, nonlinearity=nonlinearity, **k)\n",
    "    if use_batchnorm:\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "    return _\n",
    "def res_block(_, num_filters, name):\n",
    "    x = _\n",
    "    _ = reflect_padding_conv(_, num_filters, name=name+\"_conv1\")\n",
    "    _ = reflect_padding_conv(_, num_filters, nonlinearity=None, name=name+\"_conv2\")\n",
    "    return ElemwiseSumLayer([x, _], name=name+\"_add\")\n",
    "\n",
    "def RESNET_G(isize, nc_in=3, nc_out=3, ngf=64, fixed_input_size=True):\n",
    "    s = isize if fixed_input_size else None    \n",
    "    _ = InputLayer(shape=(None, nc_in, s, s), name='input')    \n",
    "    _ = reflect_padding_conv(_, ngf, 7, name=\"first\")    \n",
    "    for m in [2,4]:\n",
    "        _ = Conv2DLayer(_, num_filters=ngf*m, filter_size=4, stride=2, \n",
    "                           pad=1, W=conv_init, flip_filters=False, \n",
    "                           nonlinearity=rectify, name='conv_{}'.format(ngf*m))\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "    for i in range(6):\n",
    "        _ = res_block(_, ngf*4, \"res_block{}\".format(i))\n",
    "    for m in [2,1]:\n",
    "        _ = TransposedConv2DLayer(_, num_filters=ngf*m, filter_size=3, stride=2, \n",
    "                            crop=0, W=conv_init, flip_filters=True,\n",
    "                            nonlinearity=rectify, name=\"convt_{}\".format(ngf*m))\n",
    "        _ = batch_norm(_, epsilon=1e-5, gamma=gamma_init)\n",
    "        _ = SliceLayer(_, slice(0, -1),2)\n",
    "        _ = SliceLayer(_, slice(0, -1),3)    \n",
    "    _ = reflect_padding_conv(_, nc_out, 7, nonlinearity=tanh, use_batchnorm=False, name=\"output\")\n",
    "    return _"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc_in = 3\n",
    "nc_out = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "use_lsgan = True\n",
    "λ = 10 if use_lsgan else 100\n",
    "\n",
    "loadSize = 286\n",
    "imageSize = 256\n",
    "batchSize = 1\n",
    "lrD = 2e-4\n",
    "lrG = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputA (None, 3, None, None)\n",
      "inputB (None, 3, None, None)\n",
      "concat (None, 6, None, None)\n",
      "conv2d_0 (None, 64, None, None)\n",
      "conv2d_1 (None, 128, None, None)\n",
      "conv2d_1_bn (None, 128, None, None)\n",
      "conv2d_1_bn_nonlin (None, 128, None, None)\n",
      "conv2d_2 (None, 256, None, None)\n",
      "conv2d_2_bn (None, 256, None, None)\n",
      "conv2d_2_bn_nonlin (None, 256, None, None)\n",
      "conv2d_3 (None, 512, None, None)\n",
      "conv2d_3_bn (None, 512, None, None)\n",
      "conv2d_3_bn_nonlin (None, 512, None, None)\n",
      "conv2d_4 (None, 1, None, None)\n"
     ]
    }
   ],
   "source": [
    "netD = BASIC_D(nc_in, nc_out, ndf, use_sigmoid = not use_lsgan)\n",
    "for l in lasagne.layers.get_all_layers(netD):\n",
    "    print(l.name,  l.output_shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input (None, 3, 256, 256)\n",
      "None (None, 3, 262, 262)\n",
      "first (None, 64, 256, 256)\n",
      "first_bn (None, 64, 256, 256)\n",
      "first_bn_nonlin (None, 64, 256, 256)\n",
      "conv_128 (None, 128, 128, 128)\n",
      "conv_128_bn (None, 128, 128, 128)\n",
      "conv_128_bn_nonlin (None, 128, 128, 128)\n",
      "conv_256 (None, 256, 64, 64)\n",
      "conv_256_bn (None, 256, 64, 64)\n",
      "conv_256_bn_nonlin (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block0_conv1 (None, 256, 64, 64)\n",
      "res_block0_conv1_bn (None, 256, 64, 64)\n",
      "res_block0_conv1_bn_nonlin (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block0_conv2 (None, 256, 64, 64)\n",
      "res_block0_conv2_bn (None, 256, 64, 64)\n",
      "res_block0_conv2_bn_nonlin (None, 256, 64, 64)\n",
      "res_block0_add (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block1_conv1 (None, 256, 64, 64)\n",
      "res_block1_conv1_bn (None, 256, 64, 64)\n",
      "res_block1_conv1_bn_nonlin (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block1_conv2 (None, 256, 64, 64)\n",
      "res_block1_conv2_bn (None, 256, 64, 64)\n",
      "res_block1_conv2_bn_nonlin (None, 256, 64, 64)\n",
      "res_block1_add (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block2_conv1 (None, 256, 64, 64)\n",
      "res_block2_conv1_bn (None, 256, 64, 64)\n",
      "res_block2_conv1_bn_nonlin (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block2_conv2 (None, 256, 64, 64)\n",
      "res_block2_conv2_bn (None, 256, 64, 64)\n",
      "res_block2_conv2_bn_nonlin (None, 256, 64, 64)\n",
      "res_block2_add (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block3_conv1 (None, 256, 64, 64)\n",
      "res_block3_conv1_bn (None, 256, 64, 64)\n",
      "res_block3_conv1_bn_nonlin (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block3_conv2 (None, 256, 64, 64)\n",
      "res_block3_conv2_bn (None, 256, 64, 64)\n",
      "res_block3_conv2_bn_nonlin (None, 256, 64, 64)\n",
      "res_block3_add (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block4_conv1 (None, 256, 64, 64)\n",
      "res_block4_conv1_bn (None, 256, 64, 64)\n",
      "res_block4_conv1_bn_nonlin (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block4_conv2 (None, 256, 64, 64)\n",
      "res_block4_conv2_bn (None, 256, 64, 64)\n",
      "res_block4_conv2_bn_nonlin (None, 256, 64, 64)\n",
      "res_block4_add (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block5_conv1 (None, 256, 64, 64)\n",
      "res_block5_conv1_bn (None, 256, 64, 64)\n",
      "res_block5_conv1_bn_nonlin (None, 256, 64, 64)\n",
      "None (None, 256, 66, 66)\n",
      "res_block5_conv2 (None, 256, 64, 64)\n",
      "res_block5_conv2_bn (None, 256, 64, 64)\n",
      "res_block5_conv2_bn_nonlin (None, 256, 64, 64)\n",
      "res_block5_add (None, 256, 64, 64)\n",
      "convt_128 (None, 128, 129, 129)\n",
      "convt_128_bn (None, 128, 129, 129)\n",
      "convt_128_bn_nonlin (None, 128, 129, 129)\n",
      "None (None, 128, 128, 129)\n",
      "None (None, 128, 128, 128)\n",
      "convt_64 (None, 64, 257, 257)\n",
      "convt_64_bn (None, 64, 257, 257)\n",
      "convt_64_bn_nonlin (None, 64, 257, 257)\n",
      "None (None, 64, 256, 257)\n",
      "None (None, 64, 256, 256)\n",
      "None (None, 64, 262, 262)\n",
      "output (None, 3, 256, 256)\n"
     ]
    }
   ],
   "source": [
    "#netG = UNET_G(imageSize, nc_in, nc_out, ngf)\n",
    "netG = RESNET_G(imageSize, nc_in, nc_out, ngf)\n",
    "\n",
    "for l in lasagne.layers.get_all_layers(netG):\n",
    "    print(l.name,  l.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c342cb048f11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mreal_A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_var\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfake_B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mno_bn_avg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mnetG_generate\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mreal_A\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfake_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mnetD_l1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetD_l2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_layers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function.py\u001b[0m in \u001b[0;36mfunction\u001b[0;34m(inputs, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input)\u001b[0m\n\u001b[1;32m    315\u001b[0m                    \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                    \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                    output_keys=output_keys)\n\u001b[0m\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/pfunc.py\u001b[0m in \u001b[0;36mpfunc\u001b[0;34m(params, outputs, mode, updates, givens, no_default_updates, accept_inplace, name, rebuild_strict, allow_input_downcast, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m    484\u001b[0m                          \u001b[0maccept_inplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maccept_inplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m                          \u001b[0mprofile\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprofile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m                          output_keys=output_keys)\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36morig_function\u001b[0;34m(inputs, outputs, mode, accept_inplace, name, profile, on_unused_input, output_keys)\u001b[0m\n\u001b[1;32m   1837\u001b[0m                   \u001b[0mon_unused_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mon_unused_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1838\u001b[0m                   \u001b[0moutput_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1839\u001b[0;31m                   name=name)\n\u001b[0m\u001b[1;32m   1840\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchange_flags\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompute_test_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"off\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1841\u001b[0m             \u001b[0mfn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, mode, accept_inplace, function_builder, profile, on_unused_input, fgraph, output_keys, name)\u001b[0m\n\u001b[1;32m   1517\u001b[0m                         optimizer, inputs, outputs)\n\u001b[1;32m   1518\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1519\u001b[0;31m                     \u001b[0moptimizer_profile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1520\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                 \u001b[0mend_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \"\"\"\n\u001b[0;32m---> 99\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0madd_requirements\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph)\u001b[0m\n\u001b[1;32m    240\u001b[0m                     \u001b[0mnb_nodes_before\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_nodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m                     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 242\u001b[0;31m                     \u001b[0msub_prof\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    243\u001b[0m                     \u001b[0ml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                     \u001b[0msub_profs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_prof\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36moptimize\u001b[0;34m(self, fgraph, *args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0morig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0mtheano\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbasic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0morig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, fgraph, start_from)\u001b[0m\n\u001b[1;32m   2487\u001b[0m                         \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchange_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnb_imported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2488\u001b[0m                         \u001b[0mt_opt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2489\u001b[0;31m                         \u001b[0mlopt_change\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_node\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2490\u001b[0m                         \u001b[0mtime_opts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlopt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2491\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mlopt_change\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/opt.py\u001b[0m in \u001b[0;36mprocess_node\u001b[0;34m(self, fgraph, node, lopt)\u001b[0m\n\u001b[1;32m   2008\u001b[0m         \u001b[0mlopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlopt\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocal_opt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2009\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2010\u001b[0;31m             \u001b[0mreplacements\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2011\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2012\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfailure_callback\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/tensor/opt.py\u001b[0m in \u001b[0;36mlocal_fill_sink\u001b[0;34m(node)\u001b[0m\n\u001b[1;32m   1673\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1674\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0minput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1675\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1676\u001b[0m             \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1677\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mowner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/theano/gof/utils.py\u001b[0m in \u001b[0;36m__eq__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'__eq__'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdct\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m                 \u001b[0;32mdef\u001b[0m \u001b[0m__eq__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m                     return (type(self) == type(other) and\n\u001b[1;32m    195\u001b[0m                             \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprops\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from lasagne.layers import get_output, get_all_layers,get_all_params\n",
    "no_bn_avg = dict( batch_norm_update_averages=False,\n",
    "                       batch_norm_use_averages=False)\n",
    "real_A = get_all_layers(netG)[0].input_var\n",
    "fake_B = get_output(netG, **no_bn_avg)\n",
    "netG_generate =  theano.function([real_A], fake_B)\n",
    "\n",
    "netD_l1, netD_l2 = get_all_layers(netD)[:2]\n",
    "real_B = netD_l2.input_var\n",
    "\n",
    "output_D_real = get_output(netD, inputs={netD_l1: real_A, netD_l2: real_B}, **no_bn_avg)\n",
    "output_D_fake = get_output(netD, inputs={netD_l1: real_A, netD_l2: fake_B}, **no_bn_avg)\n",
    "if use_lsgan:\n",
    "    loss_fn = lambda output, target : T.mean( (output-target)**2 )\n",
    "else:\n",
    "    loss_fn = lambda output, target : -T.mean(T.log(output+1e-12)*target+T.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "loss_D_real = loss_fn(output_D_real, T.ones_like(output_D_real))\n",
    "loss_D_fake = loss_fn(output_D_fake, T.zeros_like(output_D_fake))\n",
    "loss_D = loss_D_real + loss_D_fake\n",
    "\n",
    "loss_G_fake = loss_fn(output_D_fake, T.ones_like(output_D_fake))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'real_B' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-9747ac6fd1a7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mloss_L1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfake_B\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mreal_B\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mloss_G\u001b[0m \u001b[0;34m=\u001b[0m  \u001b[0mloss_G_fake\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mλ\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mloss_L1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mparams_netD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetD\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mparams_netG\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_all_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnetG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'real_B' is not defined"
     ]
    }
   ],
   "source": [
    "loss_L1 = T.mean(abs(fake_B-real_B))\n",
    "loss_G =  loss_G_fake + λ * loss_L1\n",
    "\n",
    "params_netD = get_all_params(netD, trainable=True) \n",
    "params_netG = get_all_params(netG, trainable=True)\n",
    "\n",
    "\n",
    "optimize_G = lasagne.updates.adam(loss_G, params_netG, learning_rate=lrG, beta1=0.5)\n",
    "optimize_D = lasagne.updates.adam(loss_D, params_netD, learning_rate=lrD, beta1=0.5)\n",
    "netG_train =  theano.function([real_A, real_B], [loss_G_fake, loss_L1], updates=optimize_G)\n",
    "netD_train = theano.function([real_A, real_B], [loss_D/2], updates=optimize_D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from random import randint, shuffle\n",
    "\n",
    "def load_data(file_pattern):\n",
    "    return glob.glob(file_pattern)\n",
    "def read_image(fn, direction=0):\n",
    "    im = Image.open(fn)\n",
    "    im = im.resize( (loadSize*2, loadSize), Image.BILINEAR )\n",
    "    arr = np.array(im)/255*2-1\n",
    "    w1,w2 = (loadSize-imageSize)//2,(loadSize+imageSize)//2\n",
    "    h1,h2 = w1,w2\n",
    "    imgA = arr[h1:h2, loadSize+w1:loadSize+w2, :]\n",
    "    imgB = arr[h1:h2, w1:w2, :]\n",
    "    if randint(0,1):\n",
    "        imgA=imgA[:,::-1]\n",
    "        imgB=imgB[:,::-1]\n",
    "    if channel_first:\n",
    "        imgA = np.moveaxis(imgA, 2, 0)\n",
    "        imgB = np.moveaxis(imgB, 2, 0)\n",
    "    if direction==0:\n",
    "        return imgA, imgB\n",
    "    else:\n",
    "        return imgB,imgA\n",
    "\n",
    "data = \"edges2shoes\"\n",
    "data = \"facades\"\n",
    "direction = 0\n",
    "trainAB = load_data('pix2pix/{}/train/*.jpg'.format(data))\n",
    "valAB = load_data('pix2pix/{}/val/*.jpg'.format(data))\n",
    "assert len(trainAB) and len(valAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(dataAB, batchsize, direction=0):\n",
    "    length = len(dataAB)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None    \n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        if i+size > length:\n",
    "            shuffle(dataAB)\n",
    "            i = 0\n",
    "            epoch+=1        \n",
    "        dataA = []\n",
    "        dataB = []\n",
    "        for j in range(i,i+size):\n",
    "            imgA,imgB = read_image(dataAB[j], direction)\n",
    "            dataA.append(imgA)\n",
    "            dataB.append(imgB)\n",
    "        dataA = np.float32(dataA)\n",
    "        dataB = np.float32(dataB)\n",
    "        i+=size\n",
    "        tmpsize = yield epoch, dataA, dataB        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def showX(X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ( (X+1)/2*255).clip(0,255).astype('uint8')\n",
    "    if channel_first:\n",
    "        int_X = np.moveaxis(int_X.reshape(-1,3,imageSize,imageSize), 1, 3)\n",
    "    else:\n",
    "        int_X = int_X.reshape(-1,imageSize,imageSize, 3)\n",
    "    int_X = int_X.reshape(rows, -1, imageSize, imageSize,3).swapaxes(1,2).reshape(rows*imageSize,-1, 3)\n",
    "    display(Image.fromarray(int_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch = minibatch(trainAB, 12, direction=direction)\n",
    "_, trainA, trainB = next(train_batch)\n",
    "showX(trainA, 2)\n",
    "showX(trainB, 2)\n",
    "del train_batch, trainA, trainB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def netG_gen(A):\n",
    "    return np.concatenate([netG_generate(A[i:i+1]) for i in range(A.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "t0 = time.time()\n",
    "niter = 150\n",
    "gen_iterations = 0\n",
    "errL1 = epoch = errG = 0\n",
    "errL1_sum = errG_sum = errD_sum = 0\n",
    "\n",
    "display_iters = 500\n",
    "val_batch = minibatch(valAB, 6, direction)\n",
    "train_batch = minibatch(trainAB, batchSize, direction)\n",
    "\n",
    "while epoch < niter: \n",
    "    epoch, trainA, trainB = next(train_batch)        \n",
    "    errD,  = netD_train(trainA, trainB)\n",
    "    errD_sum +=errD\n",
    "\n",
    "    # epoch, trainA, trainB = next(train_batch)\n",
    "    errG, errL1 = netG_train(trainA, trainB)\n",
    "    errG_sum += errG\n",
    "    errL1_sum += errL1\n",
    "    gen_iterations+=1\n",
    "    if gen_iterations%display_iters==0:\n",
    "        if gen_iterations%(5*display_iters)==0:\n",
    "            clear_output()\n",
    "        print('[%d/%d][%d] Loss_D: %f Loss_G: %f loss_L1: %f'\n",
    "        % (epoch, niter, gen_iterations, errD_sum/display_iters, errG_sum/display_iters, errL1_sum/display_iters), time.time()-t0)\n",
    "        _, valA, valB = train_batch.send(6) \n",
    "        fakeB = netG_gen(valA)\n",
    "        showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "        errL1_sum = errG_sum = errD_sum = 0\n",
    "        _, valA, valB = next(val_batch)\n",
    "        fakeB = netG_gen(valA)\n",
    "        showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
