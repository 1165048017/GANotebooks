{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorflow implementation of https://phillipi.github.io/pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "#tf.logging.set_verbosity(tf.logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights initializations\n",
    "# bias are initailized as 0\n",
    "\n",
    "conv_init = tf.random_normal_initializer(stddev=0.02)\n",
    "gamma_init = tf.random_normal_initializer(stddev=0.02, mean=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeakyReLU(_):\n",
    "    return tf.maximum(_*0.2, _)\n",
    "\n",
    "def __LeakyReLU(x, leak=0.2, name=\"lrelu\"):\n",
    "     with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)\n",
    "    \n",
    "def ZeroPadding2D(_):\n",
    "    return tf.pad(_, [[0,0],[1,1],[1,1],[0,0]])\n",
    "\n",
    "class Model:\n",
    "    def __init__(self, BUILDER, inputs, outputs, scope_name=None, **kwargs):\n",
    "        self.inputs = inputs\n",
    "        self.outputs = outputs\n",
    "        self.scope_name=scope_name\n",
    "        self.kwargs =kwargs\n",
    "        self.BUILDER=BUILDER\n",
    "        self.trainable_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=self.scope_name)\n",
    "    def __call__(self, **kwargs):\n",
    "        return self.BUILDER(scope_name=self.scope_name, **self.kwargs, **kwargs).outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic discriminator\n",
    "def conv2d(_, f, *a, **k):\n",
    "    return tf.layers.conv2d(_, f, *a, kernel_initializer = conv_init, **k)\n",
    "\n",
    "def batchnorm(_, name=None):\n",
    "    return tf.contrib.layers.batch_norm(_, epsilon=1e-5, scale=True, is_training=True,  fused=True,\n",
    "                                         param_initializers = {'gamma':gamma_init})\n",
    "\n",
    "def BASIC_D(nc_in, nc_out, ndf, max_layers=3, scope_name=None, input_a=None, input_b=None):\n",
    "    reuse = None if scope_name is None else True\n",
    "    with tf.variable_scope(scope_name, \"BASIC_D\", [nc_in, nc_out, ndf, max_layers], reuse=reuse) as scope:\n",
    "        scope_name = scope.name\n",
    "        if input_a is None:\n",
    "            input_a = tf.placeholder(tf.float32,shape=(None, 256, 256, nc_in), name='input_a')\n",
    "        if input_b is None:\n",
    "            input_b = tf.placeholder(tf.float32, shape=(None, 256, 256, nc_out), name='input_b')\n",
    "        _ = tf.concat([input_a, input_b], axis=-1)\n",
    "        _ = conv2d(_, ndf, kernel_size=4, strides=2, padding=\"same\", \n",
    "                   name = 'First', activation=LeakyReLU)\n",
    "    \n",
    "        for layer in range(1, max_layers):        \n",
    "            out_feat = ndf * min(2**layer, 8)\n",
    "            _ = conv2d(_, out_feat, kernel_size=4, strides=2, padding=\"same\", \n",
    "                   use_bias=False, name = 'pyramid.{0}'.format(layer))\n",
    "            _ = batchnorm(_, name='batch_{}'.format(layer))        \n",
    "            _ = LeakyReLU(_)\n",
    "    \n",
    "        out_feat = ndf*min(2**max_layers, 8)\n",
    "        _ = ZeroPadding2D(_)\n",
    "        _ = conv2d(_, out_feat, kernel_size=4,  use_bias=False, name = 'pyramid_last') \n",
    "        _ = batchnorm(_, name='batch_last')\n",
    "        _ = LeakyReLU(_)\n",
    "    \n",
    "        # final layer\n",
    "        _ = ZeroPadding2D(_)\n",
    "        _ = conv2d(_, 1, kernel_size=4, name = 'final', activation = tf.nn.sigmoid)    \n",
    "    return Model(BASIC_D, inputs=[input_a, input_b], outputs=[_], scope_name=scope_name,\n",
    "                nc_in=nc_in, nc_out=nc_out, ndf=ndf, max_layers=max_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UNET_G(isize, nc_in=3, nc_out=3, ngf=64, fixed_input_size=True, input_a=None, scope_name=None):    \n",
    "    max_nf = 8*ngf    \n",
    "    def block(x, s, nf_in, use_batchnorm=True, nf_out=None, nf_next=None):\n",
    "        # print(\"block\",x,s,nf_in, use_batchnorm, nf_out, nf_next)\n",
    "        assert s>=2 and s%2==0\n",
    "        if nf_next is None:\n",
    "            nf_next = min(nf_in*2, max_nf)\n",
    "        if nf_out is None:\n",
    "            nf_out = nf_in\n",
    "        use_batchnorm = use_batchnorm and s>2\n",
    "        x = conv2d(x, nf_next, kernel_size=4, strides=2, use_bias=(not use_batchnorm),\n",
    "                   padding=\"same\", name = 'conv_{0}'.format(s))\n",
    "        if s>2:\n",
    "            if use_batchnorm:\n",
    "                x = batchnorm(x, name='batch_{}.1'.format(s))\n",
    "            x2 = LeakyReLU(x)\n",
    "            x2 = block(x2, s//2, nf_next)\n",
    "            x = tf.concat([x, x2], axis=-1)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = tf.layers.conv2d_transpose(x, nf_out, kernel_size=4, strides=2, \n",
    "                                       use_bias=(not use_batchnorm), padding='same',\n",
    "                            kernel_initializer = conv_init,          \n",
    "                            name = 'convt.{0}'.format(s))\n",
    "        if use_batchnorm:\n",
    "            x = batchnorm(x, name='batch_{}.2'.format(s))\n",
    "        if s <=8:\n",
    "            x = tf.layers.dropout(x, rate=0.5, training=True)\n",
    "        return x\n",
    "    \n",
    "    s = isize if fixed_input_size else None\n",
    "    reuse = None if scope_name is None else True\n",
    "    with tf.variable_scope(None, \"UNET_G\", [isize, nc_in, nc_out, ngf, fixed_input_size], reuse=reuse) as scope:\n",
    "        scope_name = scope.name\n",
    "        if input_a is None:\n",
    "            input_a = tf.placeholder(shape=(None, s, s, nc_in), dtype=tf.float32, name='input_a')        \n",
    "        _ = block(input_a, isize, nc_in, False, nf_out=nc_out, nf_next=ngf)\n",
    "        _ = tf.nn.tanh(_)\n",
    "    return Model(UNET_G, inputs=[input_a], outputs=[_], scope_name=scope_name, \n",
    "                isize=isize, nc_in=nc_in, nc_out=nc_out, ngf=ngf, fixed_input_size=fixed_input_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc_in = 3\n",
    "nc_out = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "Î» = 10\n",
    "\n",
    "loadSize = 286\n",
    "imageSize = 256\n",
    "batchSize = 1\n",
    "lrD = 2e-4\n",
    "lrG = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netD = BASIC_D(nc_in, nc_out, ndf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "netG = UNET_G(imageSize, nc_in, nc_out, ngf, input_a=netD.inputs[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_functions():\n",
    "    assert netG.inputs[0] is netD.inputs[0]\n",
    "    real_A = netG.inputs[0]\n",
    "    fake_B = netG.outputs[0]\n",
    "    def netG_generate(A, sess): \n",
    "        return sess.run(netG.outputs[0],feed_dict={real_A:A})\n",
    "    real_B = netD.inputs[1]\n",
    "    output_D_real = netD.outputs[0] #(input_a=real_A, input_b=real_B)[0]\n",
    "    output_D_fake = netD(input_a=real_A, input_b=fake_B)[0]\n",
    "\n",
    "    #loss_fn = lambda output, target : K.mean(K.binary_crossentropy(output, target))\n",
    "    loss_fn = lambda output, target : -tf.reduce_mean(tf.log(output+1e-12)*target+tf.log(1-output+1e-12)*(1-target))\n",
    "\n",
    "    loss_D_real = loss_fn(output_D_real, tf.ones_like(output_D_real))\n",
    "    loss_D_fake = loss_fn(output_D_fake, tf.zeros_like(output_D_fake))\n",
    "    loss_G_fake = loss_fn(output_D_fake, tf.ones_like(output_D_fake))\n",
    "\n",
    "\n",
    "    loss_L1 = tf.reduce_mean(tf.abs(fake_B-real_B))\n",
    "\n",
    "    loss_D = loss_D_real +loss_D_fake\n",
    "\n",
    "    optimizerD = tf.train.AdamOptimizer(lrD, beta1=0.5).minimize(loss_D, var_list=netD.trainable_weights)\n",
    "\n",
    "    loss_G = loss_G_fake   + 100 * loss_L1\n",
    "\n",
    "    optimizerG = tf.train.AdamOptimizer(lrG, beta1=0.5).minimize(loss_G, var_list=netG.trainable_weights)\n",
    "    def netD_train(A, B, sess):\n",
    "        return sess.run(\n",
    "            [optimizerD, loss_D/2],feed_dict={real_A:A, real_B:B})[1:]\n",
    "    def netG_train(A, B, sess):\n",
    "        return sess.run(\n",
    "            [optimizerG, loss_G_fake, loss_L1],feed_dict={real_A:A, real_B:B})[1:]\n",
    "    return netG_generate, netD_train, netG_train\n",
    "netG_generate, netD_train, netG_train = build_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from random import randint, shuffle\n",
    "\n",
    "def load_data(file_pattern):\n",
    "    return glob.glob(file_pattern)\n",
    "def read_image(fn, direction=0):\n",
    "    im = Image.open(fn)\n",
    "    im = im.resize( (loadSize*2, loadSize), Image.BILINEAR )\n",
    "    arr = np.array(im)/255*2-1\n",
    "    w1,w2 = (loadSize-imageSize)//2,(loadSize+imageSize)//2\n",
    "    h1,h2 = w1,w2\n",
    "    imgA = arr[h1:h2, loadSize+w1:loadSize+w2, :]\n",
    "    imgB = arr[h1:h2, w1:w2, :]\n",
    "    if randint(0,1):\n",
    "        imgA=imgA[:,::-1]\n",
    "        imgB=imgB[:,::-1]\n",
    "    if direction==0:\n",
    "        return imgA, imgB\n",
    "    else:\n",
    "        return imgB,imgA\n",
    "\n",
    "data = \"edges2shoes\"\n",
    "data = \"facades\"\n",
    "direction = 0\n",
    "trainAB = load_data('pix2pix/{}/train/*.jpg'.format(data))\n",
    "valAB = load_data('pix2pix/{}/val/*.jpg'.format(data))\n",
    "assert len(trainAB) and len(valAB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(dataAB, batchsize, direction=0):\n",
    "    length = len(dataAB)\n",
    "    epoch = i = 0\n",
    "    tmpsize = None    \n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        if i+size > length:\n",
    "            shuffle(dataAB)\n",
    "            i = 0\n",
    "            epoch+=1        \n",
    "        dataA = []\n",
    "        dataB = []\n",
    "        for j in range(i,i+size):\n",
    "            imgA,imgB = read_image(dataAB[j], direction)\n",
    "            dataA.append(imgA)\n",
    "            dataB.append(imgB)\n",
    "        dataA = np.float32(dataA)\n",
    "        dataB = np.float32(dataB)\n",
    "        i+=size\n",
    "        tmpsize = yield epoch, dataA, dataB        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def showX(X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ( (X+1)/2*255).clip(0,255).astype('uint8')\n",
    "    int_X = int_X.reshape(-1,imageSize,imageSize, 3)\n",
    "    int_X = int_X.reshape(rows, -1, imageSize, imageSize,3).swapaxes(1,2).reshape(rows*imageSize,-1, 3)\n",
    "    display(Image.fromarray(int_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_batch = minibatch(trainAB, 6, direction=direction)\n",
    "_, trainA, trainB = next(train_batch)\n",
    "showX(trainA)\n",
    "showX(trainB)\n",
    "del train_batch, trainA, trainB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def netG_gen(A):\n",
    "    return np.concatenate([netG_generate(A[i:i+1], sess) for i in range(A.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto()\n",
    "config.graph_options.optimizer_options.global_jit_level = tf.OptimizerOptions.ON_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "t0 = time.time()\n",
    "niter = 50\n",
    "gen_iterations = 0\n",
    "errL1 = epoch = errG = 0\n",
    "errL1_sum = errG_sum = errD_sum = 0\n",
    "\n",
    "display_iters = 500\n",
    "val_batch = minibatch(valAB, 6, direction)\n",
    "train_batch = minibatch(trainAB, batchSize, direction)\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    while epoch < niter: \n",
    "        epoch, trainA, trainB = next(train_batch)        \n",
    "        errD,  = netD_train(trainA, trainB, sess)\n",
    "        errD_sum +=errD\n",
    "\n",
    "        errG, errL1 = netG_train(trainA, trainB, sess)\n",
    "        errG_sum += errG\n",
    "        errL1_sum += errL1\n",
    "        gen_iterations+=1\n",
    "        if gen_iterations%display_iters==0:\n",
    "            if gen_iterations%(5*display_iters)==0:\n",
    "                clear_output()\n",
    "            print('[%d/%d][%d] Loss_D: %f Loss_G: %f loss_L1: %f'\n",
    "            % (epoch, niter, gen_iterations, errD_sum/display_iters, errG_sum/display_iters, errL1_sum/display_iters), time.time()-t0)\n",
    "            _, valA, valB = train_batch.send(6) \n",
    "            fakeB = netG_gen(valA)\n",
    "            showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "            errL1_sum = errG_sum = errD_sum = 0\n",
    "            _, valA, valB = next(val_batch)\n",
    "            fakeB = netG_gen(valA)\n",
    "            showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
