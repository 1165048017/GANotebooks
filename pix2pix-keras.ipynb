{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras implementation of https://phillipi.github.io/pix2pix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND']='tensorflow' # 也可以使用 theano\n",
    "os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,optimizer=fast_run,dnn.library_path=/usr/lib'\n",
    "#os.environ['THEANO_FLAGS']='floatX=float32,device=cuda,optimizer=fast_compile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Conv2D, ZeroPadding2D, BatchNormalization, Input, Dropout\n",
    "from keras.layers import Conv2DTranspose, Reshape, Activation, Cropping2D, Flatten\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.activations import relu\n",
    "from keras.initializers import RandomNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Weights initializations\n",
    "# bias are initailized as 0\n",
    "conv_init = RandomNormal(0, 0.02) # for convolution kernel\n",
    "gamma_init = RandomNormal(1., 0.02) # for batch normalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Basic discriminator\n",
    "def BASIC_D(nc, ndf, max_layers=3):\n",
    "    \"\"\"DCGAN_D(nc, ndf, max_layers=3)\n",
    "       nc: channels\n",
    "       ndf: filters of the first layer\n",
    "       max_layers: max hidden layers\n",
    "    \"\"\"    \n",
    "    use_batchnorm = True \n",
    "    _ = inputs = Input(shape=(nc, 256, 256))\n",
    "    _ = Conv2D(ndf, kernel_size=4, strides=2, padding = \"same\", kernel_initializer = conv_init, name = 'First') (_)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    for layer in range(1, max_layers):        \n",
    "        out_feat = ndf * min(2**layer, 8)\n",
    "        _ = ZeroPadding2D(1)(_)\n",
    "        _ = Conv2D(out_feat, kernel_size=4, strides=2, use_bias=(not use_batchnorm),\n",
    "                        kernel_initializer = conv_init,\n",
    "                        name = 'pyramid.{0}'.format(layer)             \n",
    "                        ) (_)\n",
    "        if use_batchnorm:\n",
    "            _ = BatchNormalization(momentum=0.9, axis=1, epsilon=1.01e-5,\n",
    "                                   gamma_initializer = gamma_init)(_, training=1)        \n",
    "        _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    out_feat = ndf*min(2**max_layers, 8)\n",
    "    _ = ZeroPadding2D(1)(_)\n",
    "    _ = Conv2D(filters=out_feat, kernel_size=4,  use_bias=(not use_batchnorm),                    \n",
    "                    kernel_initializer = conv_init,\n",
    "                    name = 'pyramid_last') (_)\n",
    "    if use_batchnorm:\n",
    "        _ = BatchNormalization(momentum=0.9, axis=1, epsilon=1.01e-5,\n",
    "                           gamma_initializer = gamma_init, \n",
    "                              )(_, training=1)\n",
    "    _ = LeakyReLU(alpha=0.2)(_)\n",
    "    \n",
    "    # final layer\n",
    "    _ = ZeroPadding2D(1)(_)\n",
    "    _ = Conv2D(filters=1, kernel_size=4,                   \n",
    "                    kernel_initializer = conv_init,\n",
    "                    name = 'final'.format(out_feat, 1),\n",
    "                    activation = \"sigmoid\"\n",
    "                    ) (_)    \n",
    "    return Model(inputs=inputs, outputs=_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def UNET_G(isize, nc_in=3, nc_out=3, ngf=64, fixed_input_size=True):    \n",
    "    conv_layers = []\n",
    "    s = isize if fixed_input_size else None\n",
    "    _ = inputs = Input(shape=(nc_in, s, s))\n",
    "    # Down sampling\n",
    "    tsize = isize    \n",
    "    #use_batchnorm = False\n",
    "    while True:\n",
    "        assert tsize>=2 and tsize%2==0\n",
    "        filters = ngf * min(2**len(conv_layers), 8)\n",
    "        use_batchnorm = len(conv_layers)>1 and tsize>2\n",
    "        _ = ZeroPadding2D(1)(_)\n",
    "        _ = Conv2D(filters, kernel_size=4, strides=2, use_bias=(not use_batchnorm),\n",
    "                   kernel_initializer = conv_init,\n",
    "                   name = 'conv.{0}'.format(len(conv_layers))\n",
    "                        ) (_)   \n",
    "        if tsize==2:\n",
    "            break        \n",
    "        if use_batchnorm:\n",
    "             _ = BatchNormalization(momentum=0.9, axis=1, epsilon=1.01e-5,\n",
    "                 gamma_initializer = gamma_init)(_, training=1)\n",
    "        conv_layers.append(_)\n",
    "        _ = LeakyReLU(alpha=0.2)(_)\n",
    "        tsize = tsize // 2\n",
    "    # Up sampling\n",
    "    use_batchnorm = True\n",
    "    while tsize<isize:\n",
    "        _ = Activation(\"relu\")(_)           \n",
    "        filters = ngf * min(2**(len(conv_layers)-1), 8)\n",
    "        _ = Conv2DTranspose(filters, kernel_size=4, strides=2, use_bias=(not use_batchnorm),\n",
    "                            kernel_initializer = conv_init,                           \n",
    "                            name = 'convt.{0}'.format(len(conv_layers)))(_)        \n",
    "        _ = Cropping2D(1)(_)\n",
    "        if use_batchnorm:\n",
    "            _ = BatchNormalization(momentum=0.9, axis=1, epsilon=1.01e-5,\n",
    "                 gamma_initializer = gamma_init)(_, training=1)\n",
    "        if tsize <=8:\n",
    "            _ = Dropout(0.5)(_, training=1)\n",
    "        _ =Concatenate(axis=1)([conv_layers.pop(), _])\n",
    "        tsize*=2\n",
    "    _ = Activation(\"relu\")(_)\n",
    "    _ = Conv2DTranspose(nc_out, kernel_size=4, strides=2,\n",
    "                            kernel_initializer = conv_init,\n",
    "                            name = 'last')(_)\n",
    "    _ = Cropping2D(1)(_)\n",
    "    _ = Activation('tanh')(_)\n",
    "    return Model(inputs=inputs, outputs=[_])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nc_in = 3\n",
    "nc_out = 3\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "λ = 10\n",
    "\n",
    "loadSize = 286\n",
    "imageSize = 256\n",
    "batchSize = 1\n",
    "lrD = 2e-4\n",
    "lrG = 2e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netD = BASIC_D(nc_in+nc_out, ndf)\n",
    "netD.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "netG = UNET_G(imageSize, nc_in, nc_out, ngf)\n",
    "netG.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import RMSprop, SGD, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "real_A = netG.input\n",
    "fake_B = netG.output\n",
    "netG_generate = K.function([real_A], [fake_B])\n",
    "\n",
    "real_B = Input(shape=(nc_out, imageSize, imageSize))\n",
    "real_AB = Concatenate(axis=1)([real_A, real_B])\n",
    "fake_AB = Concatenate(axis=1)([real_A, fake_B])\n",
    "output_D_real = netD(real_AB)\n",
    "output_D_fake = netD(fake_AB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#loss_fn = lambda output, target : K.mean(K.sum(K.binary_crossentropy(output, target), axis=[1,2,3]))\n",
    "loss_fn = lambda output, target : K.mean(K.binary_crossentropy(output, target))\n",
    "loss_D_real = loss_fn(output_D_real, K.ones_like(output_D_real))\n",
    "loss_D_fake = loss_fn(output_D_fake, K.zeros_like(output_D_fake))\n",
    "loss_G_fake = loss_fn(output_D_fake, K.ones_like(output_D_fake))\n",
    "#loss_L1 = K.mean(K.sum(K.abs(fake_B-real_B), axis=[1,2,3]))\n",
    "loss_L1 = K.mean(K.abs(fake_B-real_B))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_D = loss_D_real +loss_D_fake\n",
    "training_updates = Adam(lr=lrD, beta_1=0.5).get_updates(netD.trainable_weights,[],loss_D)\n",
    "netD_train = K.function([real_A, real_B],[loss_D/2], training_updates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "loss_G = loss_G_fake   + 100 * loss_L1\n",
    "training_updates = Adam(lr=lrG, beta_1=0.5).get_updates(netG.trainable_weights,[], loss_G)\n",
    "netG_train = K.function([real_A, real_B], [loss_G_fake, loss_L1], training_updates)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "\n",
    "def load_data(file_pattern):\n",
    "    assert nc_in == nc_out # only works when nc_in==nc_out\n",
    "    img_list = glob.glob(file_pattern)\n",
    "    L =  len(img_list)\n",
    "    dataAB  = np.zeros(shape=(L*2, 2, nc_in, imageSize, imageSize), dtype=np.float32)\n",
    "    for i, fn in enumerate(img_list):\n",
    "        im = Image.open(fn)\n",
    "        assert im.size == (512,256)\n",
    "        im = im.resize( (loadSize*2, loadSize), Image.BILINEAR )\n",
    "        arr = np.array(im)/255*2-1\n",
    "        arr = np.moveaxis(arr, 2, 0)\n",
    "        w1,w2 = (loadSize-imageSize)//2,(loadSize+imageSize)//2\n",
    "        h1,h2 = w1,w2\n",
    "        dataAB[i, 0] = arr[:, h1:h2, loadSize+w1:loadSize+w2]\n",
    "        dataAB[i, 1] = arr[:, h1:h2, w1:w2]\n",
    "    dataAB[L:] = dataAB[:L,:,:,:,::-1]\n",
    "    return dataAB\n",
    "        \n",
    "trainAB = load_data('pix2pix/facades/train/*.jpg')\n",
    "valAB = load_data('pix2pix/facades/val/*.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "def showX(X, rows=1):\n",
    "    assert X.shape[0]%rows == 0\n",
    "    int_X = ( (X+1)/2*255).clip(0,255).astype('uint8')\n",
    "    int_X = np.moveaxis(int_X.reshape(-1,3,imageSize,imageSize), 1, 3)\n",
    "    int_X = int_X.reshape(rows, -1, imageSize, imageSize,3).swapaxes(1,2).reshape(rows*imageSize,-1, 3)\n",
    "    display(Image.fromarray(int_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "showX(trainAB[:12, 0],2)\n",
    "showX(trainAB[:12, 1],2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def minibatch(dataAB, batchsize):\n",
    "    length = dataAB.shape[0]\n",
    "    epoch = i = 0\n",
    "    tmpsize = None    \n",
    "    while True:\n",
    "        size = tmpsize if tmpsize else batchsize\n",
    "        if i+size > length:\n",
    "            np.random.shuffle(dataAB)\n",
    "            i = 0\n",
    "            epoch+=1\n",
    "        dataA = dataAB[i:i+size, 0]\n",
    "        dataB = dataAB[i:i+size, 1]\n",
    "        i+=size\n",
    "        tmpsize = yield epoch, dataA, dataB        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def netG_gen(A):\n",
    "    return np.concatenate([netG_generate([A[i:i+1]])[0] for i in range(A.shape[0])], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "t0 = time.time()\n",
    "niter = 50\n",
    "gen_iterations = 0\n",
    "errL1 = epoch = errG = 0\n",
    "errL1_sum = errG_sum = errD_sum = 0\n",
    "\n",
    "display_iters = 500\n",
    "val_batch = minibatch(valAB, 6)\n",
    "\n",
    "train_batch = minibatch(trainAB, batchSize)\n",
    "while epoch < niter: \n",
    "    epoch, trainA, trainB = next(train_batch)        \n",
    "    errD,  = netD_train([trainA, trainB])\n",
    "    errD_sum +=errD\n",
    "\n",
    "    # epoch, trainA, trainB = next(train_batch)\n",
    "    errG, errL1 = netG_train([trainA, trainB])\n",
    "    errG_sum += errG\n",
    "    errL1_sum += errL1\n",
    "    gen_iterations+=1\n",
    "    if gen_iterations%display_iters==0:\n",
    "        print('[%d/%d][%d] Loss_D: %f Loss_G: %f loss_L1: %f'\n",
    "        % (epoch, niter, gen_iterations, errD_sum/display_iters, errG_sum/display_iters, errL1_sum/display_iters), time.time()-t0)\n",
    "        _, valA, valB = train_batch.send(6) \n",
    "        fakeB = netG_gen(valA)\n",
    "        showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "        errL1_sum = errG_sum = errD_sum = 0\n",
    "        _, valA, valB = next(val_batch)\n",
    "        fakeB = netG_gen(valA)\n",
    "        showX(np.concatenate([valA, valB, fakeB], axis=0), 3)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
